services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_NUM_PARALLEL=2
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - LOG_LEVEL=debug
    volumes:
      - $storage/ollama/models:/models
      - $storage/ollama/data:/root/.ollama
    gpus: "all"
    # cpus: 12.0
    # NOTE: limiting the memory prevents us to load `nous-hermes2-mixtral`
    # mem_limit: 16g
